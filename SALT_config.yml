sam:
  img_size: 512
  num_classes: 2
  sam_type: "base"

img_type: 'image'
arch: "Prompt Adapted SAM 2"
use_fdn: False
decoder_training: 'none'
mlp_transform: False

prompts:
  USE_TEXT_PROMPT: True
  NUM_TEXT_REPEAT: 1
  USE_IMAGE_PROMPT: False
  USE_SLICE_NUM: False
  LOCATION: 'prepend'
  DROPOUT: 0
  NUM_TOKENS: 5


decoder_training: none
training:
  optimizer: 'adamw'
  lr: 1e-4
  batch_size: 8
  num_epochs: 200
  schedule_step: 200
  warmup_steps: 1000
  schedular: #put your schedular here
  steps: [5000, 10000]
  decay_factor: 0.1 
  schedule_step_factor: 0.2
  weight_decay: 1e-2
  loss: 'focal+dice'
  reg_multiplier: 0

#TODO: implement logic to parse this params.
use_salt: False
ft:
  # type: SALT_LoRA_adapt_shift_ciai
  type: svd
  svd_rank_linear: 0
  svd_rank_conv2d: 0
  r_lora: 4



#Large
Hiera:
  stages: [2, 6, 36, 4]
  window_pos_embed_bkg_spatial_size: [7, 7]
  window_spec: [8, 4, 16, 8]
  fpn_top_down_levels: [2, 3]
  global_att_blocks: [23, 33, 43]
  backbone_channel_list: [1152, 576, 288, 144]

#Base
Hiera:
  fpn_top_down_levels: [2, 3]
  backbone_channel_list: [896, 448, 224, 112]
# use_lora: False